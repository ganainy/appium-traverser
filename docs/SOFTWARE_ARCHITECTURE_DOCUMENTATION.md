# AI-Driven Android App Crawler - Software Architecture Documentation

Status: Development

## Table of Contents
1. [Project Overview](#project-overview)
2. [System Architecture](#system-architecture)
3. [Core Components](#core-components)
4. [Data Flow](#data-flow)
5. [Configuration Management](#configuration-management)
6. [Database Schema](#database-schema)
7. [AI Integration](#ai-integration)
8. [Development Guidelines](#development-guidelines)
9. [Extending the System](#extending-the-system)
10. [Testing Strategy](#testing-strategy)
11. [Deployment & Operations](#deployment--operations)

## Project Overview

### Purpose
The AI-Driven Android App Crawler is an automated testing and exploration tool that uses pluggable AI model adapters (Gemini, OpenRouter, Ollama) to intelligently navigate Android applications. It analyzes visual layouts and structural information to make decisions about the next best action for discovering new states and interactions.

### Key Features
- **AI-Powered Exploration**: Uses provider adapters (Gemini, OpenRouter, Ollama) for intelligent action selection
- **Intelligent State Management**: Visual and structural hashing to identify unique screens
- **Loop Detection**: Prevents getting stuck in repetitive patterns
- **Traffic Capture**: Optional network monitoring via PCAPdroid
- **CLI Interface**: Command-line interface for automation and scripting
- **Comprehensive Reporting**: PDF reports with crawl analysis
- **Configurable**: Extensive options to customize crawler behavior

### Technology Stack
- **Language**: Python 3.8+
- **Mobile Automation**: MCP Server with Appium backend and UIAutomator2
- **AI Models**: Pluggable provider adapters (Gemini, Ollama, OpenRouter)
- **Database**: SQLite
- **Image Processing**: Pillow, ImageHash
- **GUI Framework**: PySide6 (Qt)
 - **PDF Generation**: xhtml2pdf (optional; install with `pip install xhtml2pdf`)
- **Network Capture**: PCAPdroid integration

### Interfaces & Entry Points
- **CLI Controller**: run via `python run_cli.py`
- **UI Controller**: run via `python run_ui.py`

## Project Architecture

This section provides a concise overview of the project's structure and where to find in-depth details:

- Core components and responsibilities are described in the "Core Components" section below.
- High-level diagrams and layering are covered in "System Architecture".
- End-to-end data and output paths are covered in "Data Flow".
- Configuration layers and dynamic updates are documented in "Configuration Management".

## System Architecture

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         User Interface Layer                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  CLI Controller (run_cli.py)  â”‚  UI Controller (run_ui.py)  â”‚  Configuration Files  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Core Orchestration                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    Shared Orchestration Layer                   â”‚
â”‚                         main.py                                 â”‚
â”‚                        crawler.py                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Component Layer                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Agent Assistant â”‚  State Mgmt   â”‚  Action Exec  â”‚  App Context  â”‚
â”‚  (Adapters: Gemini/OpenRouter/Ollama) â”‚  (Hashing)    â”‚  (MCP Client) â”‚  (Lifecycle)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Screenshot    â”‚  Traffic      â”‚  Action       â”‚  Database     â”‚
â”‚   Annotator     â”‚  Capture      â”‚  Mapper       â”‚  Manager      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Infrastructure Layer                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  MCP Server    â”‚  Android Device  â”‚  SQLite DB  â”‚  File System â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Interaction Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CLI         â”‚â”€â”€â”€â–¶â”‚ Main        â”‚â”€â”€â”€â–¶â”‚ Crawler     â”‚â”€â”€â”€â–¶â”‚ AI          â”‚
â”‚ Controllers â”‚    â”‚ Entry Point â”‚    â”‚ Orchestrate â”‚    â”‚ Assistant   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            â”‚                    â”‚
                                            â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Database    â”‚â—€â”€â”€â”€â”‚ State       â”‚â—€â”€â”€â”€â”‚ Screenshot  â”‚â—€â”€â”€â”€â”‚ MCP         â”‚
â”‚ Manager     â”‚    â”‚ Manager     â”‚    â”‚ Capture     â”‚    â”‚ Client      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                               â”‚
                                                               â–¼
                                                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                     â”‚ MCP Server   â”‚
                                                     â”‚ (Appium      â”‚
                                                     â”‚ Backend)     â”‚
                                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                               â”‚
                                                               â–¼
                                                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                     â”‚ Android     â”‚
                                                     â”‚ Device      â”‚
                                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Core Components

### 1. Main Entry Point (`main.py`)
**Purpose**: Bootstrap and orchestrate the entire crawling process.

**Key Responsibilities**:
- Initialize configuration and logging
- Set up output directories
- Create and manage AppCrawler instance
- Handle graceful shutdown and cleanup

**Configuration Dependencies**:
```python
required_config = [
    'GEMINI_API_KEY', 'APP_PACKAGE', 'APP_ACTIVITY',
    'OUTPUT_DATA_DIR', 'SHUTDOWN_FLAG_PATH'
]
```

### 2. AppCrawler (`crawler.py`)
**Purpose**: Core orchestration engine that manages the crawling lifecycle.

**Key Responsibilities**:
- Coordinate all components (AI, state management, actions)
- Implement crawling loop logic
- Handle termination conditions (time/steps)
- Manage pause/resume functionality
- Error handling and recovery

**State Machine**:
```
INITIALIZING â†’ CONNECTING â†’ LAUNCHING_APP â†’ CRAWLING â†’ FINALIZING â†’ COMPLETED
                                             â†‘         â†“
                                           PAUSED â†â”€â”€â”€â”€â”˜
```

**Critical Methods**:
```python
async def run_async(self) -> None:
    """Main crawling loop with AI decision making"""

def _should_terminate(self) -> bool:
    """Check termination conditions (time/steps/failures)"""

def _step_crawl_action(self) -> Tuple[bool, Optional[str]]:
    """Execute one crawling step with AI guidance"""
```

### 3. Shared Orchestration Layer (`core/`)

The Shared Orchestration Layer provides common functionality used by both CLI and UI interfaces:

#### Key Components

**CrawlerOrchestrator** (`core/controller.py`)
- Main orchestrator for crawler lifecycle management
- Handles process launch, monitoring, and termination
- Provides unified interface for both CLI and UI

**CrawlerLaunchPlan** (`core/controller.py`)
- Encapsulates all parameters needed to launch a crawler process
- Includes execution parameters, configuration, and validation results

**Process Backends** (`core/adapters.py`)
- `SubprocessBackend`: For CLI environments using subprocess
- `QtProcessBackend`: For UI environments using QProcess
- Abstracts process handling differences between environments

**FlagController** (`core/controller.py`)
- Manages flag files for crawler process control (shutdown, pause)
- Provides unified flag management for both CLI and UI

**ValidationService** (`core/validation.py`)
- Provides shared validation utilities for pre-flight checks
- Validates MCP server, MobSF, Ollama, and API key configurations

#### Benefits

1. **Code Reuse**: Eliminates duplication between CLI and UI implementations
2. **Consistency**: Ensures consistent behavior across interfaces
3. **Maintainability**: Centralized orchestration logic is easier to maintain
4. **Testability**: Shared components can be tested independently

#### Usage Example

```python
# Create orchestrator with appropriate backend
backend = create_process_backend(use_qt=False)  # For CLI
orchestrator = CrawlerOrchestrator(config, backend)

# Start crawler
success = orchestrator.start_crawler()

# Get status
status = orchestrator.get_status()

# Stop crawler
orchestrator.stop_crawler()
```

### 4. Agent Assistant (`agent_assistant.py`)
**Purpose**: Primary decision-making agent that integrates with provider-specific model adapters (Gemini, OpenRouter, Ollama) and enforces structured prompting tailored for mobile UI testing.

**Key Responsibilities**:
- Initialize and manage provider adapters and selected models
- Analyze screenshots and UI XML to craft context-rich prompts
- Enforce structured JSON output schema for actions and reasoning
- Maintain and optionally persist chat memory across steps
- Apply safety settings and provider-specific constraints
- Handle provider failures and implement fallbacks/backoffs

**Decision Process** (conceptual):
```python
def choose_next_action(
    screenshot_input: Union[bytes, str],  # bytes or image path (normalized by adapter)
    xml_content: str,
    previous_actions: List[str],
    available_actions: List[str],
    current_visits: int,
    screen_hash: str
) -> Tuple[Optional[Dict], Optional[str]]:
    """
    Returns a structured action recommendation and optional error.
    Delegates model invocation through the selected provider adapter.
    """
```

**AI Input Format**:
- **Visual**: Screenshot image (PNG bytes or file path, depending on provider)
- **Structural**: XML UI hierarchy
- **Context**: Previous actions, visit counts, available actions
- **Constraints**: Allowed action types, target app packages

### 3a. Model Adapters (`model_adapters.py`)
**Purpose**: Provide a unified integration layer across AI providers.

**Key Responsibilities**:
- Normalize request/response formats across providers
- Detect and enforce vision capability (image input support)
- Map display names to raw model aliases and handle provider-specific quirks
- Manage provider-specific configuration and payload limits

### 4. Screen State Manager (`screen_state_manager.py`)
**Purpose**: Track unique screen states and manage state transitions.

**Key Responsibilities**:
- Generate composite hashes (XML + visual)
- Detect screen transitions and loops
- Maintain visit counts and action history
- Store screen representations in database

**Hashing Strategy**:
```python
def _get_composite_hash(self, xml_hash: str, visual_hash: str) -> str:
    return f"{xml_hash}_{visual_hash}"

# Visual similarity detection
def _calculate_visual_hash(self, screenshot_bytes: bytes) -> str:
    # Using ImageHash for perceptual hashing
```

**State Representation**:
```python
@dataclass
class ScreenRepresentation:
    screen_id: int
    composite_hash: str
    xml_hash: str
    visual_hash: str
    package_name: str
    activity_name: str
    screenshot_path: str
    xml_content: str
    timestamp: float
```

### 5. MCP Client (`appium_driver.py`)
**Purpose**: MCP client wrapper that routes device actions through MCP server tools.

**Key Responsibilities**:
- Connect to MCP server via HTTP/WebSocket
- Route action requests to MCP tools (tap, input_text, scroll, etc.)
- Handle MCP protocol communication
- Provide high-level action methods compatible with existing code

**MCP Server Connection**:
```python
# MCP client connects to server running Appium backend
mcp_client = MCPClient(server_url="http://localhost:3000")
```

### 6. Action Components

#### Action Mapper (`action_mapper.py`)
**Purpose**: Map AI suggestions to executable MCP tool calls.

**Finding Strategies** (Priority Order):
1. **ID** (resource-id): prefers full resource-id; falls back to contains(@resource-id, ...) XPath if needed
2. **Accessibility ID** (content-desc)
3. **Text Case-Insensitive** (XPath translate-based contains on @text)
4. **Content-Desc Case-Insensitive** (XPath translate-based contains on @content-desc)
5. **Class Contains Match** (XPath contains on class)

Heavier XPath heuristics like "XPath Contains" and "XPath Flexible" are only enabled when `DISABLE_EXPENSIVE_XPATH` is False in configuration.

#### Action Executor (`action_executor.py`)
**Purpose**: Execute mapped actions via MCP tool calls with error handling and fallbacks.

**Supported Actions**:
- `click`: Tap on UI elements via MCP tap tool
- `input`: Text input via MCP input_text tool
- `scroll_down/up`: Vertical scrolling via MCP scroll tool
- `swipe_left/right`: Horizontal swiping via MCP swipe tool
- `back`: Android back button via MCP back tool

### 7. Database Manager (`database.py`)
**Purpose**: Manage SQLite database for crawl data persistence.

**Schema Tables**:
- `runs`: Crawl session metadata
- `screens`: Unique screen states
- `steps_log`: Individual crawl actions with AI/mapping context
- `transitions`: Simplified transitions between screens
- `run_meta`: Per-run metadata snapshots

### 8. Traffic Capture Manager (`traffic_capture_manager.py`)
**Purpose**: Integrate with PCAPdroid for network traffic monitoring.

**Workflow**:
1. Start PCAPdroid capture via ADB intent
2. Filter traffic by target app package
3. Pull PCAP files from device
4. Clean up device storage

### 9. Configuration System (`config.py`)
**Purpose**: Centralized configuration management with layered overrides.

**Configuration Layers** (Priority Order):
1. User config file (`user_config.json`)
2. Environment variables
3. Default values

## Data Flow

### Crawling Step Flow
```
1. Screenshot Capture
   â”œâ”€ Get current screen image (PNG bytes)
   â”œâ”€ Extract XML UI hierarchy
   â””â”€ Detect current app package/activity

2. State Analysis
   â”œâ”€ Generate visual hash (ImageHash)
   â”œâ”€ Generate XML structure hash
   â”œâ”€ Create composite hash
   â””â”€ Check if screen is new/visited

3. AI Decision Making
   â”œâ”€ Send screenshot + XML to selected AI provider (via adapter)
   â”œâ”€ Include context (previous actions, visits)
   â”œâ”€ Get structured JSON response
   â””â”€ Parse action recommendation

4. Action Execution
   â”œâ”€ Map AI suggestion to MCP tool call
   â”œâ”€ Send request to MCP server
   â”œâ”€ Execute action via Appium backend
   â””â”€ Apply coordinate fallback if needed

5. State Update
   â”œâ”€ Capture post-action screenshot
   â”œâ”€ Update visit counts and history
   â”œâ”€ Record step in database
   â””â”€ Check termination conditions
```

### Data Storage Flow
```
Raw Data Collection (session-scoped):
â”œâ”€ Screenshots â†’ {session_dir}/screenshots/
â”œâ”€ Annotated Screenshots â†’ {session_dir}/annotated_screenshots/
â”œâ”€ Traffic Captures â†’ {session_dir}/traffic_captures/
â””â”€ Logs â†’ {session_dir}/logs/

Database Storage:
â”œâ”€ Screen States â†’ screens table
â”œâ”€ Crawl Steps â†’ steps_log table
â”œâ”€ Transitions â†’ transitions table
â””â”€ Run Metadata â†’ run_meta table

Analysis Output:
â”œâ”€ PDF Reports â†’ {session_dir}/reports/
â”œâ”€ UI Element Annotations (offline, per-session) â†’ {session_dir}/annotated_screenshots/ (image overlays)
â””â”€ Optional annotation JSONs (if generated by tools) â†’ stored alongside session outputs
```

## Configuration Management

### Configuration Structure
```python
class Config:
    # Core Application Settings
    APP_PACKAGE: str = "com.example.app"
    APP_ACTIVITY: str = "com.example.MainActivity"
    MCP_SERVER_URL: str = "http://127.0.0.1:3000"
    
    # AI Configuration (provider-agnostic)
    GEMINI_API_KEY: str = ""  # From environment or .env
    OPENROUTER_API_KEY: str = ""
    OLLAMA_BASE_URL: str = "http://127.0.0.1:11434"
    AI_PROVIDER: str = "gemini"  # "gemini" | "openrouter" | "ollama"
    DEFAULT_MODEL_TYPE: str = "flash-latest-fast"
    MAX_CHAT_HISTORY: int = 10
    
    # Crawling Behavior
    CRAWL_MODE: str = "steps"  # "steps" or "time"
    MAX_CRAWL_DURATION_SECONDS: int = 600
    MAX_CRAWL_STEPS: int = 10
    
    # State Management
    VISUAL_SIMILARITY_THRESHOLD: int = 5
    CONTINUE_EXISTING_RUN: bool = False
    
    # Error Handling
    MAX_CONSECUTIVE_AI_FAILURES: int = 3
    MAX_CONSECUTIVE_MAP_FAILURES: int = 3
    MAX_CONSECUTIVE_EXEC_FAILURES: int = 3
    
    # Paths (resolved per session)
    OUTPUT_DATA_DIR = "output_data"
    SESSION_DIR = "{output_data_dir}/{device_id}_{app_package}_{timestamp}"
    SCREENSHOTS_DIR = "{session_dir}/screenshots"
    ANNOTATED_SCREENSHOTS_DIR = "{session_dir}/annotated_screenshots"
    TRAFFIC_CAPTURE_OUTPUT_DIR = "{session_dir}/traffic_captures"
    LOG_DIR = "{session_dir}/logs"
    DB_NAME = "{session_dir}/database/{package}_crawl_data.db"
    PDF_REPORT_DIR = "{session_dir}/reports"

    # XPath tuning
    DISABLE_EXPENSIVE_XPATH: bool = False
```

### Configuration Loading Priority
1. **Default Values** (in `config.py`)
2. **Environment Variables** (`.env` file)
3. **User Configuration** (`user_config.json`)

### Dynamic Configuration Updates
```python
# CLI Updates
python run_cli.py --set-config MAX_CRAWL_STEPS=15

# Programmatic Updates
config.update_setting_and_save('MAX_CRAWL_STEPS', 15)
```

## Database Schema

### Tables Structure

#### `runs` Table
```sql
CREATE TABLE IF NOT EXISTS runs (
    run_id INTEGER PRIMARY KEY AUTOINCREMENT,
    app_package TEXT NOT NULL,
    start_activity TEXT NOT NULL,
    start_time DATETIME DEFAULT CURRENT_TIMESTAMP,
    end_time DATETIME,
    status TEXT DEFAULT 'STARTED'
);
```

#### `screens` Table
```sql
CREATE TABLE IF NOT EXISTS screens (
    screen_id INTEGER PRIMARY KEY AUTOINCREMENT,
    composite_hash TEXT NOT NULL UNIQUE,
    xml_hash TEXT NOT NULL,
    visual_hash TEXT NOT NULL,
    screenshot_path TEXT,
    activity_name TEXT,
    xml_content TEXT,
    first_seen_timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    first_seen_run_id INTEGER,
    first_seen_step_number INTEGER
);
```

#### `steps_log` Table
```sql
CREATE TABLE IF NOT EXISTS steps_log (
    step_log_id INTEGER PRIMARY KEY AUTOINCREMENT,
    run_id INTEGER NOT NULL,
    step_number INTEGER NOT NULL,
    from_screen_id INTEGER,
    to_screen_id INTEGER,
    action_description TEXT,
    ai_suggestion_json TEXT,
    mapped_action_json TEXT,
    execution_success BOOLEAN,
    error_message TEXT,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    ai_response_time_ms REAL,
    total_tokens INTEGER
);
```

#### `transitions` Table
```sql
CREATE TABLE IF NOT EXISTS transitions (
    transition_id INTEGER PRIMARY KEY AUTOINCREMENT,
    from_screen_id INTEGER NOT NULL,
    to_screen_id INTEGER,
    action_description TEXT NOT NULL,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
);
```

#### `run_meta` Table
```sql
CREATE TABLE IF NOT EXISTS run_meta (
    meta_id INTEGER PRIMARY KEY AUTOINCREMENT,
    run_id INTEGER NOT NULL,
    meta_json TEXT,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
);
```

## AI Integration

### Google Gemini Integration Architecture

#### Model Configuration
```python
MODEL_CONFIGS = {
    "gemini-2.5-flash-image": {
        "model_name": "gemini-2.5-flash-image",
        "description": "Gemini 2.5 Flash, optimized for fast multimodal responses",
        "generation_config": {
            "temperature": 0.3,
            "top_p": 0.8,
            "top_k": 20,
            "max_output_tokens": 2048
        }
    }
}
```

#### Structured Output Schema
```python
ACTION_SCHEMA = {
    "type": "object",
    "properties": {
        "action": {"type": "string", "enum": ["click", "input", "scroll_down", "scroll_up", "swipe_left", "swipe_right", "back"]},
        "identifier": {"type": "string"},
        "input_text": {"type": "string"},
        "reasoning": {"type": "string"}
    },
    "required": ["action", "reasoning"]
}
```

#### Prompt Engineering Strategy
```python
def build_action_prompt(
    xml_content: str,
    previous_actions: List[str],
    available_actions: List[str],
    current_visits: int,
    screen_hash: str
) -> str:
    """Build context-aware prompt for AI decision making"""
    
    prompt_parts = [
        "SYSTEM_ROLE": "Android app exploration expert",
        "TASK": "Analyze screenshot and decide next action",
        "CONTEXT": f"Screen visited {current_visits} times",
        "PREVIOUS": f"Last actions: {previous_actions}",
        "AVAILABLE": f"Available actions: {available_actions}",
        "XML_STRUCTURE": xml_content,
        "CONSTRAINTS": "Avoid loops, explore new features",
        "OUTPUT_FORMAT": "JSON with action and reasoning"
    ]
```

### AI Decision Quality Assurance

#### Response Validation
```python
def validate_ai_response(response: Dict) -> Tuple[bool, Optional[str]]:
    """Validate AI response structure and content"""
    required_fields = ["action", "reasoning"]
    valid_actions = ["click", "input", "scroll_down", "scroll_up", "swipe_left", "swipe_right", "back"]
    
    # Structure validation
    if not all(field in response for field in required_fields):
        return False, "Missing required fields"
    
    # Action validation
    if response["action"] not in valid_actions:
        return False, f"Invalid action: {response['action']}"
    
    # Context validation
    if response["action"] == "input" and not response.get("input_text"):
        return False, "Input action requires input_text"
```

#### Fallback Strategies
1. **AI Failure**: Use predefined exploration patterns
2. **Action Mapping Failure**: Try coordinate-based actions
3. **Execution Failure**: Implement retry with different strategies

## Development Guidelines

### Code Organization Principles

#### Separation of Concerns
- **Configuration**: Centralized in `config.py`
- **Business Logic**: Core crawling in `crawler.py`
- **External Integrations**: Separate modules (AI, Appium, Database)
- **User Interface**: CLI in dedicated controller

#### Error Handling Strategy
```python
class CrawlerException(Exception):
    """Base exception for crawler-specific errors"""
    pass

class AIResponseError(CrawlerException):
    """Raised when AI response is invalid or unusable"""
    pass

class ActionMappingError(CrawlerException):
    """Raised when AI action cannot be mapped to UI element"""
    pass

class DeviceConnectionError(CrawlerException):
    """Raised when device communication fails"""
    pass
```

#### Logging Standards
```python
# Use structured logging with context
logging.info(
    f"Step {step_num}: AI suggested {action} on '{target}'. "
    f"Reasoning: {reasoning}"
)

# Include performance metrics
logging.info(
    f"AI API call completed. Processing Time: {elapsed:.2f}s, "
    f"Tokens: {prompt_tokens}+{response_tokens}={total_tokens}"
)
```

### Testing Strategy

#### Unit Testing Structure
```
tests/
â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ test_ai_assistant.py
â”‚   â”œâ”€â”€ test_state_manager.py
â”‚   â”œâ”€â”€ test_action_mapper.py
â”‚   â””â”€â”€ test_config.py
â”œâ”€â”€ integration/
â”‚   â”œâ”€â”€ test_crawler_flow.py
â”‚   â””â”€â”€ test_database_operations.py
â””â”€â”€ e2e/
    â””â”€â”€ test_full_crawl_session.py
```

#### Mock Strategy
```python
# Mock AI responses for testing
@patch('ai_assistant.AIAssistant.get_next_action')
def test_crawler_step_execution(mock_ai):
    mock_ai.return_value = ({
        "action": "click",
        "identifier": "login_button",
        "reasoning": "Navigate to login screen"
    }, None)
    
    # Test crawler step execution
    success, message = crawler._step_crawl_action()
    assert success is True
```

#### Integration Testing
```python
def test_full_crawl_integration():
    """Test complete crawl flow with real Appium but mocked AI"""
    # Setup test app and device
    # Configure short crawl duration
    # Execute crawl
    # Verify expected outputs (screenshots, database, reports)
```

### Performance Considerations

#### Memory Management
- **Screenshot Caching**: Limit cached images to prevent memory leaks
- **Database Connections**: Use connection pooling for concurrent access
- **AI Response Caching**: Cache similar screens to reduce API calls

#### Optimization Strategies
```python
# Async operations for I/O
async def capture_screenshot_async():
    """Non-blocking screenshot capture"""

# Lazy loading for large data
def get_screen_xml_lazy(screen_id: int):
    """Load XML content only when needed"""

# Batch database operations
def batch_insert_steps(steps: List[StepRecord]):
    """Insert multiple steps in single transaction"""
```

## Extending the System

### Adding New AI Models

#### 1. Model Configuration
```python
# In config.py
MODEL_CONFIGS["new-model"] = {
    "model_name": "new-ai-model-name",
    "description": "Description of new model",
    "generation_config": {
        "temperature": 0.1,
        # Model-specific parameters
    }
}
```

#### 2. Model Adapter Implementation
```python
class NewModelAdapter(AIModelAdapter):
    """Adapter for new AI model integration"""
    
    def generate_action(self, prompt: str, image: bytes) -> Dict:
        """Implement model-specific API call"""
        pass
    
    def validate_response(self, response: Any) -> Dict:
        """Convert model response to standard format"""
        pass
```

### Adding New Action Types

#### 1. Action Definition
```python
# In action_executor.py
SUPPORTED_ACTIONS = [
    "click", "input", "scroll_down", "scroll_up",
    "swipe_left", "swipe_right", "back",
    "long_press",  # New action
    "double_tap"   # New action
]
```

#### 2. Action Implementation
```python
def execute_long_press(self, element: WebElement, duration: int = 2000) -> bool:
    """Execute long press action on element"""
    try:
        self.driver.long_press(element, duration)
        return True
    except Exception as e:
        logging.error(f"Long press failed: {e}")
        return False
```

#### 3. AI Integration
```python
# Update AI prompt to include new actions
def get_available_actions(self, context: Dict) -> List[str]:
    """Get context-appropriate actions"""
    base_actions = ["click", "input", "scroll_down", "back"]
    
    # Add contextual actions
    if context.get("has_long_pressable_elements"):
        base_actions.append("long_press")
    
    return base_actions
```

### Adding New Analysis Features

#### 1. Analysis Module
```python
class PerformanceAnalyzer:
    """Analyze crawl performance metrics"""
    
    def analyze_coverage(self, run_id: int) -> Dict:
        """Calculate screen coverage metrics"""
        pass
    
    def analyze_efficiency(self, run_id: int) -> Dict:
        """Calculate crawling efficiency"""
        pass
```

#### 2. Report Integration
```python
# In analysis_viewer.py
def generate_enhanced_report(self, run_id: int) -> str:
    """Generate report with new analysis"""
    
    # Existing analysis
    basic_analysis = self.generate_basic_analysis(run_id)
    
    # New analysis
    performance_analysis = PerformanceAnalyzer().analyze_coverage(run_id)
    
    # Combine in report
    return self.combine_analyses([basic_analysis, performance_analysis])
```

### Plugin Architecture (Future Enhancement)

#### Plugin Interface
```python
class CrawlerPlugin:
    """Base class for crawler plugins"""
    
    def on_step_start(self, context: StepContext) -> None:
        """Called before each crawl step"""
        pass
    
    def on_step_complete(self, context: StepContext, result: StepResult) -> None:
        """Called after each crawl step"""
        pass
    
    def on_crawl_complete(self, context: CrawlContext) -> None:
        """Called after crawl completion"""
        pass
```

#### Plugin Registration
```python
class PluginManager:
    """Manage crawler plugins"""
    
    def __init__(self):
        self.plugins: List[CrawlerPlugin] = []
    
    def register_plugin(self, plugin: CrawlerPlugin) -> None:
        """Register a new plugin"""
        self.plugins.append(plugin)
    
    def trigger_event(self, event: str, *args, **kwargs) -> None:
        """Trigger event on all plugins"""
        for plugin in self.plugins:
            getattr(plugin, event)(*args, **kwargs)
```

## Deployment & Operations

### Environment Setup

#### Production Environment
```bash
# System dependencies
sudo apt-get install python3.8 python3-pip nodejs npm openjdk-8-jdk

# Android SDK setup
export ANDROID_HOME=/opt/android-sdk
export PATH=$PATH:$ANDROID_HOME/platform-tools

# Appium setup
npm install -g appium@2.17.1
appium driver install uiautomator2
```

#### Container Deployment
```dockerfile
FROM python:3.8-slim

# Install system dependencies
RUN apt-get update && apt-get install -y \
    android-tools-adb \
    nodejs npm \
    && rm -rf /var/lib/apt/lists/*

# Install Appium
RUN npm install -g appium@2.17.1
RUN appium driver install uiautomator2

# Install Python dependencies
COPY requirements.txt .
RUN pip install -r requirements.txt

# Copy application code
COPY . /app
WORKDIR /app

# Run crawler
CMD ["python", "run_cli.py"]
```

### Monitoring & Alerting

#### Health Checks
```python
class HealthChecker:
    """Monitor crawler health and performance"""
    
    def check_appium_connection(self) -> bool:
        """Verify Appium server is accessible"""
        pass
    
    def check_device_connection(self) -> bool:
        """Verify Android device is connected"""
        pass
    
    def check_ai_api_status(self) -> bool:
        """Verify AI API is responding"""
        pass
    
    def check_database_integrity(self) -> bool:
        """Verify database is accessible and intact"""
        pass
```

#### Metrics Collection
```python
class MetricsCollector:
    """Collect and export crawler metrics"""
    
    def record_crawl_duration(self, duration: float) -> None:
        """Record total crawl time"""
        pass
    
    def record_ai_response_time(self, response_time: float) -> None:
        """Record AI API response times"""
        pass
    
    def record_action_success_rate(self, success_rate: float) -> None:
        """Record action execution success rate"""
        pass
```

### Scaling Considerations

#### Horizontal Scaling
- **Device Pools**: Multiple Android devices for parallel crawling
- **Appium Grid**: Distributed Appium nodes for load balancing
- **Database Sharding**: Separate databases per app or time period

#### Resource Management
```python
class ResourceManager:
    """Manage system resources during crawling"""
    
    def limit_memory_usage(self, max_mb: int) -> None:
        """Set memory usage limits"""
        pass
    
    def throttle_ai_requests(self, requests_per_minute: int) -> None:
        """Implement AI API rate limiting"""
        pass
    
    def manage_disk_space(self, max_gb: int) -> None:
        """Clean up old screenshots and logs"""
        pass
```

### Backup & Recovery

#### Data Backup Strategy
```python
class BackupManager:
    """Manage crawler data backups"""
    
    def backup_database(self, backup_path: str) -> bool:
        """Create database backup"""
        pass
    
    def backup_screenshots(self, backup_path: str) -> bool:
        """Archive screenshot collections"""
        pass
    
    def restore_from_backup(self, backup_path: str) -> bool:
        """Restore crawler data from backup"""
        pass
```

#### Disaster Recovery
1. **Database Corruption**: Automatic backup validation and restoration
2. **Device Failure**: Automatic device detection and failover
3. **AI API Outage**: Fallback to rule-based exploration
4. **Network Issues**: Retry mechanisms with exponential backoff

---

## Conclusion

This documentation provides a comprehensive foundation for understanding, maintaining, and extending the AI-Driven Android App Crawler. The modular architecture and clear separation of concerns make it possible to enhance individual components without affecting the overall system stability.

For new developers joining the project, start by:
1. Setting up the development environment using the setup guide
2. Running the example crawls to understand the workflow
3. Examining the test suite to understand expected behaviors
4. Contributing to specific components based on your expertise area

For questions or contributions, please refer to the project's issue tracker and contribution guidelines.

---

## Modular CLI Architecture

### Overview

The project includes a refactored modular CLI architecture that replaces the monolithic `cli_controller.py` with a clean, maintainable structure. The new architecture provides better separation of concerns, improved testability, and easier extensibility.

### Architecture

#### Project Structure
```
traverser_ai_api/cli/
â”œâ”€â”€ __init__.py              # Package initialization
â”œâ”€â”€ main.py                  # Main entry point and command orchestration
â”œâ”€â”€ parser.py                # Argument parser builder
â”œâ”€â”€ shared/                  # Shared utilities and context
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ context.py           # CLIContext and ServiceRegistry
â”‚   â””â”€â”€ serializers.py       # JSON serialization utilities
â”œâ”€â”€ services/                # Business logic services
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config_service.py    # Configuration management
â”‚   â”œâ”€â”€ process_utils.py     # Process and PID management
â”‚   â””â”€â”€ telemetry.py         # Logging and status reporting
â””â”€â”€ commands/                # CLI command implementations
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ base.py              # Base command infrastructure
    â”œâ”€â”€ config.py            # Configuration commands
    â””â”€â”€ services_check.py    # Service validation commands
```

#### Key Components

1. **CLIContext**: Central dependency injection container providing Config, LoggerManager, and ServiceRegistry
2. **CommandHandler Protocol**: Standard interface for all commands with `register()` and `run()` methods
3. **Service Layer**: Business logic separation with ConfigService, ProcessUtils, and TelemetryService
4. **Command Groups**: Logical grouping of related commands (device, apps, crawler, etc.)

#### Command Structure
- **Standalone Commands**: `show-config`, `set-config`, `precheck-services`
- **Command Groups**: `device list`, `apps scan-health`, `crawler start`, etc.

#### Entry Points
- **Modular CLI**: `python run_cli.py` (recommended)

#### Benefits
- **Maintainability**: Clear separation of concerns and modular structure
- **Extensibility**: Easy to add new commands and services
- **Testability**: Dependency injection enables proper unit testing
- **User Experience**: Hierarchical commands with better help organization

#### Migration Status
| Legacy Command | New Command | Status |
|----------------|--------------|---------|
| `--show-config` | `show-config` | âœ… Implemented |
| `--set-config K=V` | `set-config K=V` | âœ… Implemented |
| `--precheck-services` | `precheck-services` | âœ… Implemented |
| `--list-devices` | `device list` | ðŸš§ Planned |
| `--scan-health-apps` | `apps scan-health` | ðŸš§ Planned |
| `--start` | `crawler start` | ðŸš§ Planned |

For detailed documentation, see `docs/MODULAR_CLI_ARCHITECTURE.md`.
